调度系统开发排期 
Quartz+任务管理4天 ( quartz 2 任务状态管理 2 )   多一天
HA控制和YARN状态感知3天 ( 2 1 ) 
任务解析器包括HDFS、HIVE、MR、Spark和Shell大概5天 (每个1)    
报警 1天 
前端交互2天 
自测 1天 
共16天


## 异常
改变scheduleMap的状态会异常
Exception in thread "schedulerWatcherThread" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437)
	at java.util.HashMap$EntryIterator.next(HashMap.java:1471)
	at java.util.HashMap$EntryIterator.next(HashMap.java:1469)
	at com.naixue.dp.scheduler.SchedulerWatcher.loadSchedule(SchedulerWatcher.java:141)
	at com.naixue.dp.scheduler.SchedulerWatcher.run(SchedulerWatcher.java:404)
	at java.lang.Thread.run(Thread.java:748)


7 参数
"java", "-jar", Constants.TASKRUNNER, this.command, this.task_id.toString(), this.query_name.toString(), String.valueOf(this.scripttype_id), Constants.TASKRUNNER_LOG, Constants.TASKRUNNER_CONFIG, get_categroy()

java -Dfile.encoding=UTF-8 -jar /home/hadoop/dwetl/dw_general_loader.jar 29512
java -Dfile.encoding=UTF-8 -jar /home/hadoop/dwetl/dw_general_loader.jar 2128
java -Dfile.encoding=UTF-8 -jar /home/hadoop/dwetl/dw_general_loader.jar 28533

" --mainjar secondarysort.jar \n --mainclass com.szw.test.Test \n --mainargs /user/hadoop/README /user/hadoop/o\n --param mapreduce.input.fileinputformat.split.maxsize=1\n --param mapreduce.input.fileinputformat.split.maxsize=2"

--mainjar secondarysort.jar 
--mainclass com.szw.test.Test 
--mainargs /user/hadoop/README /user/hadoop/o
--param mapreduce.input.fileinputformat.split.maxsize=1
--param mapreduce.input.fileinputformat.split.maxsize=2

## 手动执行时，需要手动将任务信息写入zk ready queue  --- 手动执行是不进入ready queue的
手动执行的话，将job信息写入zk ready queue和db

* __全局变量__ MR oterArgs是指什么
* 不能调度秒级的任务 
* 每个job同时只能有一个实例在执行  
* 每个job同时只能有一个实例处于wait状态  也就是说只要有某个实例的状态是未完成的，则不许有新的实例出现
* 运行未结束，下一次调度开始       重复提交    禁止 报警
	怎么判断？
* 在QuartzJob中判断当前NameSpace下所有jobId的调度状态和执行状态，如果有正在调度或者正在执行的实例，则此次调度失败    属于  重复提交   报警
* // 是否需要executorId，需要，同一个namespace中同一jobId可以运行很多次，
  // jobId和namespace并不能唯一确认一条信息
* 报警时还得判断该job有没配置报警
* 优先级越大级别越高
* kill job 直接去磁盘拿pid，然后kill
	rpc 给我executorId，后端kill，返回值 (后端kill的话也是去拿pid文件，然后kill)
* 如果从一开始调度就把executorId带上呢。。。   从getWait时带上
	需要一个query_name，因为executeId是一个自增的主键，无法插入，如果后期直接从schedule状态到read状态时拿不到executeId，需要生成一个query_name标识一个job实例的唯一性
* 重跑疑问
	- (父任务执行成功的前提下)子任务正在执行，重跑父任务-- 子任务不会有任何操作
	- 并发重跑

* namespace - 调度区间 (区间内查找完成的job)  类似调度批次  调度周期  一个调度流
	- 某批任务一天执行n次，每次依赖检测检查当前批次的任务状态


## TODO
强制重跑
自动重试
修复bug 用户手机号为null
并发执行(默认并发重跑现在)  --  是否判断前一个任务的执行状态   上次调度未结束，此次调度已开始，是等待吗？
小时类任务 -- 此次调度是否需要关注下上小时的有没有调度完成

bug 卡住  -- sql异常
	jstack -l 11047
	jmap -dump:live,format=b,file=heap.bin 11047
	jhat -J-Xmx1024m heap.bin  
bug 修改job的调度时间之后未调度  因为map里是以jobId为key，修改了调度时间并未更新scheduleMap
删除文件

全局变量

kill hive

mysql 8小时问题 
2018-03-01 17:27:04  [ com.naixue.dp.util.DbUtil.createStatement(DbUtil.java:77):readyWatcherThread ] - [ ERROR ]  The last       packet successfully received from the server was 83,937,883 milliseconds ago.  The last packet sent successfully to the server       was 83,937,884 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expi      ring and/or testing connection validity before use in your application, increasing the server configured values for client time      outs, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem. with - select type from t_dict_j      ob_type where id = 3

2018-03-05 19:30:10 INFO  TriggerManager:362 - Doing trigger actions Execute flow hdfs2 from project szw_cyclical for Trigger        Id: 4, Description: Trigger from SimpleTimeTrigger with trigger condition of BasicTimeChecker_1.eval() and expire condition of        EndTimeChecker_1.eval(), Execute flow hdfs2 from project szw_cyclical
186887 2018-03-05 19:30:10 INFO  ExecuteFlowAction:239 - Invoking flow szw_cyclical.hdfs2
186888 2018-03-05 19:30:10 INFO  ExecutorManager:939 - Submitting execution flow hdfs2 by work
186889 2018-03-05 19:30:10 INFO  ExecutionFlowDao:72 - Flow given hdfs2 given id 8991
186890 2018-03-05 19:30:10 INFO  ExecutorManager:1489 - Successfully dispatched exec 8991 with error count 0
186891 2018-03-05 19:30:10 INFO  ExecuteFlowAction:241 - Invoked flow szw_cyclical.hdfs2
186892 2018-03-05 19:30:10 INFO  Condition:122 - Done resetting checkers. The next check time will be 2018-03-05T19:35:00.000+08:00
186893 2018-03-05 19:30:10 INFO  JdbcTriggerImpl:138 - Updating trigger 4 into db.



出错阻塞
禁止重跑
凌晨任务
延迟报警

依赖重跑  --  调度比较频繁的任务之间依赖

任务恢复  --  服务异常之后如何快速恢复

未上线的任务不能被依赖，下线时需要进行依赖检测，先把依赖关掉然后才能下线任务
循环依赖
上线状态也可以操纵去除依赖？？？
前端控制重跑，避免同一job误提交多次重跑

# env
export HADOOP_HOME=/opt/hadoop/
export HIVE_HOME=/opt/hive/
export SQOOP_HOME=/opt/sqoop/
export HBASE_HOME=/opt/hbase/
export PATH={$PATH}:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$HBASE_HOME/bin

## 后端所用字段
job_type 是不是应该有个字典表    1:数据抽取脚本, 2:MySQL脚本, 3:hive脚本, 4:MR脚本, 5:shell脚本, 7:spark脚本, 8:storm脚本, 9:Wcrontab, 10:WTable Dump, 12:Spark-Streaming, 13:HDFS-Check, 14:MiniReport
schedule add jar_path 存放jar的hdfs目录

## 问题 
hive任务提交时，jobname的设置？
hive等任务的killjob

HADOOP COMMAND:hadoop jar /home/hadoop/dp-scheduler-worker/dp-scheduler/work_dir/10630.jar -Dmapreduce.job.queuename=root.offline.hdp_ubu_zhuanzhuan.normal -D58.department=15247923_转转技术部 -D58.user=夏保根 -Dwbdp.job.name=zzStatisticLogETL -Dwbdp.job.execute.id=15247923 /home/hdp_ubu_zhuanzhuan/rawdata/wf:zzentry,zzpay,zzwebactivity,zzwebadmin,zzkfwebadmin,zzyywebadmin,zzcrmentry,muyingweb,userrecall,ershou_youpin_m,ershou_v_youpin,ershou_youpin,ershou_h_youpin,zzczextensionweb,jwdf_m_sale,ershou_recycle,zzopenentry,zzcorecyclewf;/home/hdp_ubu_zhuanzhuan/rawdata/scf:zzactivity,zzadmin,zzbusiness,zzbusinesslogic,zzbusinessmobile,zzconfig,zzcount,zzdispatch,zzinfo,zzinfologic,zzlogic,zzpush,zzreclogic,zzrecommend,zzsearch,zzsocial,zzsociallogic,zzspam,zztrade,zztradelogic,zzuser,zzvillage,zzgroup,zzgrouplogic,zzmoment,zzmomentlogic,zzpost,muying,zzusercenter,zzpassport,zzyylogic,zzinfoshow,zzpushlogic,zzpaymentcenter,muyinght,zzcreateorder,zzthirdpartlogic,zzinfoshowbase,zzproduct,zzinfolike,zzinfoconfig,zzsuggestprice,zzminiapplogic,zzminiappcommon,ershou_zzpublicserver,zzvisitrecommend,zzmarketlogic,zzopenextend,zzwords,ershou_quality,zzmarketjob,zzfeature,zzyyrecommend,zzershouserver,ershou_ypSku,wxunitpushchannel,ershou_ypdeal,zzgrouplist,zzgroupstatics,realtimemodel,realtimefeature,zzppu,zzbizinfoserver,zzbizpayserver,zzbizadserver,zzrefundlogic,zzbook,zzbooklogic,zzarbitrationlogic,zzcorecycle;/home/hdp_ubu_zhuanzhuan/rawdata/node:zzactivity;/home/hdp_ubu_zhuanzhuan/rawdata/task:zzminiapp_mobilerecycle,zzmqq /home/hdp_ubu_zhuanzhuan/resultdata/server/zzStatisticLogExtract 2018-02-26 70

## mvn
D:\apache-maven-3.5.2\bin\mvn clean package -X
java -jar target\schedulerExecutor-1.0-SNAPSHOT-runnable.jar com.naixue.dp.executor.Executor

java -jar schedulerExecutor-1.0-SNAPSHOT.jar

## 前后端交互
* 字段设置为非null，后端插入时必须得把该字段插入
* 重跑
	- 向t_job_execute_log表中插入一条数据，包括dispatch_namespace[格式：yyyy-MM-dd]和queryName[格式：jobId_System.currentTimeMillis]
* 读log -- 目录地址 在executor.properties中SCRIPT_LOG_PATH配置[脚本执行日志目录和文件名前缀--目录结构为 logs/script_logs_queryName_process_info]
	- 213机器上的配置地址为/home/work/sunzhiwei/log/tmp_scripts
	- process_log存在程序的过程日志
	- result_log存在system_out信息


* job_state(两个表更新) 和 job_status&&schedule_status
*  namespace

## 图
title schedule state

Schedule_waiting->Schedule_ready: SchedulerWatcher
Schedule_ready->Schedule_running: ReadyWatcher
Schedule_running->Job_submit: executor
Job_submit->Job_running
Job_running->Job_success

## 内置变量
${tempCatalog}	/tmp/dw_tmp_file/	临时目录
${outFileSuffix}	2014-04-03，如果手动选择，则为选择日期-1天	导出文件名的后缀，昨天日期
${startDate}	'2014-04-03'	昨天日期，如果手动选择，则为选择日期-1天
${dateSuffix}	20140403	默认任务执行的前一天，如果手动选择，则为选择日期-1天
${dateHourSuffix}	2015061117	当前日期小时后缀
${dateBeforeOneHourSuffix}	2015061116	当前日期前一小时后缀
${monthSuffix}	201404	月份后缀
${lastMonthSuffix}	201403	上个月份后缀
?	2014-04-03	以JDBC参数方式传入，昨天日期，废弃使用
${today}	'2015-08-24'	当日，如果手动选择，则为选择日期
${todayDateTime}	yyyy-MM-dd HH:mm:ss	当前日期时间，如果手动选择，则为选择日期时间
${todaySuffix}	20150824	当日，如果手动选择则为选择日期
${dealDate}	'2014-04-03'	昨日，如果手动选择，则为选择日期-1天
${monthId}	'2014M04'	昨日的月，如果手动选择，则为选择日期-1天的月份
${monthBegin}	'2014-04-01'	昨日的月初，如果手动选择，则为选择日期-1天的月初
${monthEnd}	'2014-04-30'	昨日的月末，如果手动选择，则为选择日期-1天的月末
${monthBeginSuffix}	20140401	昨日的月初，如果手动选择，则为选择日期-1天的月初
${monthEndSuffix}	20140430	昨日的月末，如果手动选择，则为选择日期-1天的月末
${weekId}	'2014W15'	昨日周WeekId，如果手动选择，则为选择日期-1天的周WeekId
${weekBegin}	'2014-04-06'(周日)	跨年了，该值就变成20**-01-01，如果手动选择，则为选择日期-1天的周日
${weekEnd}	'2014-04-12'(周六)	跨年了，该值变成20**-12-31
${weekBeginSuffix}	20140406(周日)	跨年了，该值就变成20**0101
${weekEndSuffix}	20140412(周六)	跨年了，该值变成20**1231
${weekBeginCn}	'2015-08-24'(周一)	昨日相对的周一，如果手动选择，则为选择日期-1天的周一
${weekEndCn}	'2015-08-30'(周日)	昨日相对的周日，如果手动选择，则为选择日期-1天的周日
${weekBeginCnSuffix}	20150824(周一)	
${weekEndCnSuffix}	20150830(周日)	
${sevenDaysBefore}	date_sub(?,interval 7 day)	取七天前日期
${sevenDaysBeforeSuffix}	日期格式是 yyyyMMdd	取7天前日期
${thirtyDaysBeforeSuffix}	日期格式是 yyyyMMdd	取30天前日期
${sixtyDaysBeforeSuffix}	日期格式是 yyyyMMdd	取60天前日期
${monthOnlySuffix}	monthOfYear	日期的月份(06)
$bash{date +%Y%m%d -d '-1 hour'}	{}中的bash日期表达式用户自定义	按表达式解析为准
${dataFile}	按一定规则生成，含module_name	数据文件路径


# db cache

4	发布-未绑芝麻每小时发布	7	1	qy01256013c725b200288b246402	1、未绑定芝麻信用分 2、发布成功 3、指定品类		{
  "fileName": "statistic_1.0_SNAPSHOT.jar", 
  "newFileName": "statistic_1.0_SNAPSHOT_1517391175314.jar", 
  "filePath": "/Users/wangyu/Downloads/files/jar/statistic_1.0_SNAPSHOT_1517391175314.jar", 
  "uploadFlag": "0", 
  "jobPackage": "", 
  "serverId": "1", 
  "inputPath": "/home/hdp_ubu_zhuanzhuan/resultdata/dw/allGoodsInfo/;/home/hdp_ubu_zhuanzhuan/resultdata/wangyu27/xianyu_mobile/", 
  "outputPath": "/home/hdp_ubu_zhuanzhuan/resultdata/wangyu27/xianyu_similarity2/",
  "otherArgs": "${outFileSuffix} ${outFileSuffix}", 
  "scheduler": { 
    "status": "1",
    "jobLevel": "2",
    "hadoopQueueId": "1",
    "retry": "3",
    "errorRunContinue": "1",
    "parallelRun": "0",
    "isMonitor": "0",
    "delayWarn": "0",
    "receiver": "qy010060a7c77fb2002841d22acd",
    "runTime": "0 05 1 * * ? *"
  },
  "dependencies": "34472" 
}	0			2018-01-19 12:36:06	2018-02-09 16:27:34
5	测试123	3	2	qy01256013c725b200288b246402	123		{
  "fileName": "GoodsSimilarityOptimize2.jar",
  "newFileName": "GoodsSimilarityOptimize2_1517391561752.jar",
  "filePath": "/Users/wangyu/Downloads/files/jar/GoodsSimilarityOptimize2_1517391561752.jar", 
  "uploadFlag": "0",
  "jobPackage": "com.naixue.online.info_fe_codis_sets", 
  "serverId": "1", 
  "sparkMaster": "yarn-cluster", 
  "sparkVersion": "2.0", 
  "numExecutors": "10", 
  "executorCores": "5", 
  "executorMemory": "20", 
  "sparkOtherArgs": "--files /usr/lib/software/spark/spark-2.0/conf/hive-site.xml\n--conf spark.dynamicAllocation.maxExecutors=30\n--conf spark.dynamicAllocation.minExecutors=10\n--driver-memory 10G",
  "mainClassArgs": "brandcate2price,cate2ctr,cate2pricectr,cate3ctr,cate3pricectr,citycate2price,gendercate1ctr,gendercate2ctr,gendercate3ctr genderitembrandctr,genderitemwordctr,sourcectr,terminalcate2price,userbrandctr,usercate1ctr,usercate2ctr,usercate2pricectr,usercate3ctr", 
  "scheduler": {
    "status": "1",
    "jobLevel": "2",
    "hadoopQueueId": "1",
    "retry": "0",
    "errorRunContinue": "0",
    "parallelRun": "0",
    "isMonitor": "0",
    "delayWarn": "1",
    "receiver": "qy010060a7c77fb2002841d22acd",
    "runTime": "0 0 0 1 1 ? 2015"
  },
  "dependencies": "" 
}				2018-01-30 15:56:53	2018-02-24 11:57:24




add jar hdfs://hdp-58-cluster/home/hdp_ubu_zhuanzhuan/resultdata/wangyu27/conf/MysqlOutput.jar;
add jar hdfs://hdp-58-cluster/home/hdp_ubu_zhuanzhuan/resultdata/wangyu27/conf/mysql-connector-java-5.1.6.jar;
create temporary function dboutput as "dbOutput.MysqlOutputUDF";

set mapreduce.job.queuename=root.offline.hdp_ubu_zhuanzhuan.normal;
set hive.exec.reducers.max=40;

-- select dboutput("jdbc:mysql://10.48.186.32:3306/db_zhuanzhuan","root","UDP@mj505","replace into t_miniprog_idle_componsate(stat_date, group_id, group_name, grant_num, use_num, qualified_num, componsate_num, componsate_amount) values(?, ?, ?, ?, ?, ?, ?, ?)", stat_date, group_id, name, grant_num, use_num, qualified_num, componsate_num, componsate_amount) from (
select dboutput("jdbc:mysql://10.126.103.26:3306/db_zz_goods","zeye","zeye@BI0824","replace into t_miniprog_idle_componsate(stat_date, group_id, group_name, grant_num, use_num, qualified_num, componsate_num, componsate_amount) values(?, ?, ?, ?, ?, ?, ?, ?)", stat_date, group_id, name, grant_num, use_num, qualified_num, componsate_num, componsate_amount) from (
select "${outFileSuffix}" stat_date, dict.group_id, name, nvl(grant_num, 0) as grant_num, nvl(use_num, 0) as use_num, nvl(qualified_num, 0) as qualified_num, nvl(componsate_num, 0) as componsate_num, nvl(componsate_amount, 0) as componsate_amount from 
(
/** 修改记录
 * 2018-01-09 by wangyu （李丹）只需要已签约的闲置社数据，`signed_status` tinyint(2) DEFAULT "1" COMMENT "签约状态  1 已签约",（备注： 未签约的不一定全在次表中，以group_core中type=1  去掉已签约的闲置社为准。）
**/
	select core.group_id, core.name from hdp_ubu_zhuanzhuan_defaultdb.t_group_core_new core join  hdp_ubu_zhuanzhuan_defaultdb.t_group_extend_info extends on core.group_id=extends.group_id  where core.type=1 and extends.signed_status=1
	union all 
	select -1 group_id, "全部" name
) dict
left join 
(
	select nvl(group_id, -1) as group_id, 
	sum(if(action="grant", num, 0)) grant_num, -- 发放特权数
	sum(if(action="use", num, 0)) use_num, -- 使用特权数
	sum(if(action="qualified", num, 0)) qualified_num, -- 满足赔付条件商品数
	sum(if(action="compensate", num, 0)) componsate_num, -- 赔付商品数
	round(sum(if(action="compensate_amount", num, 0))/100, 2) componsate_amount -- 赔付总金额
	from 
	(
		-- 每天发放特权数
		select "grant" as action, grant_biz_id as group_id, 1 as num from hdp_ubu_zhuanzhuan_defaultdb.t_privilege_user2 where date="${dateSuffix}" and scheme_id=3 and scheme_type=6 and state>=1 and grant_time>=unix_timestamp("${dateSuffix}", "yyyyMMdd")*1000 and grant_time<unix_timestamp(date_add("${outFileSuffix}", 1), "yyyy-MM-dd")*1000
		union all 
		-- 使用特权数（领取特权后发布商品）
		SELECT "use" as action, group_id, 1 as num from hdp_ubu_zhuanzhuan_defaultdb.t_zzserver_action LATERAL view json_tuple(datapool, "command", "groupId") json as command, group_id where date="${outFileSuffix}" and module="zzgrouplogic" and servertype="scf" and action="addSpareInfo" and command="validpostguarantee"
		union all 
		-- 满足赔付条件商品数（领取特权后发布商品并满足赔付条件）
		select "qualified" as action, grant_biz_id as group_id, 1 as num from hdp_ubu_zhuanzhuan_defaultdb.t_privilege_user2 where date="${dateSuffix}" and scheme_id=3 and scheme_type=6 and consume_state>=1 and consume_time>=unix_timestamp("${dateSuffix}", "yyyyMMdd")*1000 and consume_time<unix_timestamp(date_add("${outFileSuffix}", 1), "yyyy-MM-dd")*1000
		union all 
		-- 赔付商品数
		select "compensate" as action, grant_biz_id as group_id, 1 as num from hdp_ubu_zhuanzhuan_defaultdb.t_privilege_user2 where date="${dateSuffix}" and scheme_id=3 and scheme_type=6 and consume_state=3 and finish_time>=unix_timestamp("${dateSuffix}", "yyyyMMdd")*1000 and finish_time<unix_timestamp(date_add("${outFileSuffix}", 1), "yyyy-MM-dd")*1000 
		union all 
		-- 赔付总金额
		select "compensate_amount" as action, group_id, amount as num from hdp_ubu_zhuanzhuan_defaultdb.t_zzserver_action LATERAL view json_tuple(datapool, "schemeId", "schemeType", "amount", "groupId") json as scheme_id, scheme_type, amount, group_id where date="${outFileSuffix}" and module="zzminiapplogic" and action="privilegePaySuccess" and scheme_id=3 and scheme_type=6 
	) tb group by group_id WITH cube
) tb 
on tb.group_id=dict.group_id) tb;